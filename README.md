# ğŸš€ LLM AccelGo
**Fine-Tuning and Accelerating Large Language Models (LLMs) on GPUs using Go, CUDA, and TensorRT**

## ğŸ§  Overview
**LLM AccelGo** demonstrates how Go can drive GPU-accelerated machine learning workflowsâ€”covering fine-tuning, distributed training, and optimized inference for LLMs.
Built on **DigitalOcean AI GPU droplets**, the project showcases **mixed precision**, **LoRA adapters**, and **8-bit optimizers**, achieving significant speedups and memory savings while serving models through a scalable Go-based API.

## ğŸ¯ Key Features
- âš™ï¸ **Go + CUDA Integration** â€“ GPU kernels via `cgo`, leveraging cuBLAS and Tensor Cores
- ğŸ§© **LLM Fine-Tuning in Go** â€“ using GoTorch/Gorgonia for Transformer training
- ğŸª¶ **LoRA & 8-bit Optimizers** â€“ lightweight adaptation with reduced VRAM footprint
- âš¡ **Mixed Precision (FP16/INT8)** â€“ improved throughput and lower memory consumption
- ğŸ” **Distributed Training** â€“ NCCL-based multi-GPU gradient synchronization
- ğŸŒ **Optimized Inference Server** â€“ TensorRT/ONNX Runtime accelerated inference via Go REST API
- â˜ï¸ **Cloud-Native Deployment** â€“ built, trained, and served on DigitalOcean GPU instances

## ğŸ§± Architecture
```
LLM-AccelGo/
â”‚
â”œâ”€â”€ train/           # GoTorch-based fine-tuning logic
â”œâ”€â”€ infer/           # Model inference (ONNX/TensorRT backend)
â”œâ”€â”€ serve/           # Go HTTP REST API server
â”œâ”€â”€ cuda/            # Custom CUDA kernels (via cgo)
â”œâ”€â”€ scripts/         # Setup, benchmarking, and deployment scripts
â””â”€â”€ results/         # Logs, metrics, and visualizations
```

## âš™ï¸ Setup

### 1ï¸âƒ£ Provision GPU Instance
- Use a **DigitalOcean AI GPU Droplet** (e.g., RTX 6000 Ada / A100)
- Select **AI image** with NVIDIA drivers pre-installed

### 2ï¸âƒ£ Install Dependencies
```bash
sudo apt update && sudo apt install -y golang cuda-toolkit-12-4
go install github.com/wangkuiyi/gotorch@latest
```

### 3ï¸âƒ£ Verify CUDA + Go Integration
```bash
go run cuda/vector_add.go
```
âœ… Should print GPU-computed vector addition results.

## ğŸ§© Training Example
```bash
go run train/main.go --model llama2-7b --precision fp16 --lora true --optimizer adam8bit
```
Outputs epoch-wise GPU utilization, time per batch, and loss metrics.

## âš¡ Inference & Serving
```bash
go run serve/main.go --model ./models/llama2-7b-lora.onnx --backend tensorrt
```
REST API endpoint:
```bash
POST /generate
{ "prompt": "Explain CUDA memory hierarchy" }
```

## ğŸ“Š Benchmark Results
| Mode | Precision | Speedup | VRAM (GB) | Latency (ms) |
|------|------------|----------|------------|---------------|
| Baseline | FP32 | 1.0Ã— | 15.8 | 210 |
| Mixed Precision | FP16 | 1.8Ã— | 9.1 | 120 |
| LoRA + FP16 | FP16 | 1.9Ã— | 7.2 | 105 |
| Quantized | INT8 | 2.2Ã— | 5.6 | 60 |

## ğŸ“š References
- Hu et al., *LoRA: Low-Rank Adaptation of LLMs* (2021)
- Dettmers et al., *8-bit Optimizers for Efficient LLM Training* (2022)
- Dao et al., *FlashAttention* (NeurIPS 2022)
- NVIDIA Developer Blog, *Optimizing LLMs for Performance and Accuracy* (2025)

## ğŸ§¾ Citation / Attribution
If you use this work in your research or project, please cite:
```
@project{LLMAccelGo2025,
  author = {Ketan Sarda},
  title  = {LLM AccelGo: Fine-Tuning and Accelerating Large Language Models in Go},
  year   = {2025},
  note   = {DigitalOcean GPU + CUDA + TensorRT implementation}
}
```

## ğŸ‘¨â€ğŸ’» Author
**Ketan Sarda**
- ğŸ“ M.S. Computer Science, UC Irvine
- ğŸ’¼ AI Architecture Intern, Lenovo | SQL DBA, Regis Aged Care
- ğŸŒ [linkedin.com/in/ketan-sarda](https://linkedin.com/in/ketan-sarda)
